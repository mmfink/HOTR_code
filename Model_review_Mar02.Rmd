---
title: "Latest Iteration of HOTR model evaluation"
author: "Michelle M. Fink"
date: "03/03/2020"
output:
  html_document:
    code_folding: hide
    df_print: kable
    fig_align: center
    fig_height: 6
    highlight: tango
    theme: readable
  pdf_document:
    fig_crop: no
    fig_height: 6
    highlight: tango
    keep_tex: yes
    theme: readable
---
```{r setup, include=FALSE}
library(knitr)
library(kableExtra)
knitr::opts_chunk$set(cache = TRUE, cache.lazy = FALSE)
```

Expand code block to see libraries and functions used.
```{r message=FALSE, warning=FALSE, cache=FALSE}
library(dplyr)
library(data.table)
library(stringr)
library(randomForest)
library(lme4)
library(ROCR)
library(PresenceAbsence) # for Kappa and PCC
library(ggplot2)

getConfusionMatrix <- function(tbl) {
  #https://stats.stackexchange.com/questions/35609/why-do-i-need-bag-composition-to-calculate-oob-error-of-combined-random-forest-m/35613#35613
  class.error = vector()
  
  for (i in 1:nrow(tbl)) {
    rowSum = sum(tbl[i,])
    accurate = diag(tbl)[i]
    error = rowSum - accurate
    
    class.error[i] = error / rowSum
  }   
  return(cbind(tbl, class.error))
}

opt.cut <- function(perf, pred){
  #https://www.r-bloggers.com/a-small-introduction-to-the-rocr-package/
  #calculates sensitivity = specificity threshold
  mapply(FUN=function(x, y, p){
    d = (x - 0)^2 + (y-1)^2
    ind = which(d == min(d))
    c(sensitivity = y[[ind]], specificity = 1-x[[ind]], 
      cutoff = p[[ind]])
  }, perf@x.values, perf@y.values, pred@cutoffs)
}

modEval <- function(modobj, run_name, modtype='GLM'){
  if(modtype=="RF"){
    p_rf.mod <- predict(modobj, type = "prob")
    pred.mod <- prediction(p_rf.mod[,2], modobj$y)
  } else {
    pred.mod <- prediction(modobj@resp$mu, modobj@resp$y)
  }
  
  perf.roc <- performance(pred.mod, "tpr", "fpr")
  perf.err <- performance(pred.mod, "err")
  AUC <- performance(pred.mod, measure = "auc")@y.values[[1]]
  sst <- opt.cut(perf.roc,pred.mod)
  TSS <- sst[1] + sst[2] - 1
  
  if(modtype=="RF"){
    ptab_rf.mod <- table(modobj$predicted, modobj$y)
    err_rate <- 1-sum(diag(ptab_rf.mod))/sum(ptab_rf.mod)
  } else {
    err_rate <- perf.err@y.values[[1]][which(perf.err@x.values[[1]] == sst[3])]
  }
  
  #Presence.Absence insists on its own data structure
  padat <- data.frame(ID=seq(1:length(pred.mod@predictions[[1]])),
                    OBS=as.numeric(levels(pred.mod@labels[[1]]))[pred.mod@labels[[1]]],
                    PRED=pred.mod@predictions[[1]])

  pacmx <- cmx(padat, threshold = sst[3])
  kappa <- Kappa(pacmx, st.dev = FALSE)
  PCC <- pcc(pacmx, st.dev = FALSE)
  
  #plotting ROC
  #ggplot code adapted from https://davidrroberts.wordpress.com/2015/09/22/quick-auc-function-in-r-with-rocr-package/
  i <- which(perf.roc@alpha.values[[1]] == sst[3])
  plotdat <- data.frame(FP=perf.roc@x.values[[1]],TP=perf.roc@y.values[[1]])
  rocp <- ggplot(plotdat, aes(x=FP,y=TP)) +
    geom_line(lwd=2, col="red") +
    geom_abline(intercept=0,slope=1,lwd=1,lty=2,col="gray") +
    geom_point(aes(x=perf.roc@x.values[[1]][i], y=perf.roc@y.values[[1]][i]), 
               pch=19, size=3, col="blue") +
    annotate("text",x=0.97,y=0.15,label=str_glue("AUC={round(AUC,3)}"),hjust=1) +
    annotate("text",x=0.97,y=0.10,label=str_glue("Threshold={round(sst[3],3)}"),hjust=1) +
    scale_x_continuous("False Positive Rate", limits=c(0,1)) +
    scale_y_continuous("True Positive Rate", limits=c(0,1)) +
    coord_fixed(ratio = 1) +
    ggtitle(paste(run_name, "ROC"))

  return(list(rocp, c(run_name, AUC, TSS, err_rate, kappa, PCC, sst[1], sst[2], sst[3]), pacmx))
}
```



# Results of the different runs {.tabset .tabset-fade}

Note that most of these models are very large files and take a good bit of memory to load and time to analyze.  
This script can only realistically be run on the CNHP modeling server. The output html requires javascript to work.  

Each tab displays a different aspect of model evaluation and comparison. Click on the **'Code'** button to view the code blocks.  

*Load Models:*  
```{r}
pth <- "H:/HOTR_models/"
RF_cd <- readRDS(paste0(pth,"RF_climdirt_Feb25_norm.rds"))
RF_all1 <- readRDS(paste0(pth,"RF_alltypes_onehot_Feb25_norm.rds"))
RF_all2 <- readRDS(paste0(pth,"RF_alltypes_Mar02_norm.rds"))

GLM_cd <- readRDS(paste0(pth,"GLMER_climdirt_Feb25_norm.rds"))
GLM_all <- readRDS(paste0(pth,"GLMER_alltypes_Feb25_norm.rds"))
GLM_all2 <- readRDS(paste0(pth,"GLMER_alltypes_Feb25_norm.rds"))
```

*Run each model through main evaluation function*
```{r}
evRF_cd <- modEval(RF_cd, "RF Climate and Topo/Soils", modtype = "RF")
evRF_all1 <- modEval(RF_all1, "RF Climate, Topo/Soils, NLCD One-Hot", modtype = "RF")
evRF_all2 <- modEval(RF_all2, "RF Climate, Topo/Soils, NLCD", modtype = "RF")

evGLM_cd <- modEval(GLM_cd, "GLMM Climate and Topo/Soils")
evGLM_all <- modEval(GLM_all, "GLMM Climate, Topo/Soils, NLCD")
evGLM_all2 <- modEval(GLM_all2, "GLMM Climate, Topo/Soils, NLCD modified")
```

## Covariance

The graphic below shows the Covariate Correlation matrix of all of the environmental inputs that 1) were used in these models and 2) would fit within the function. (meaning; not everything is shown but I did my best)  

The output is independent of any model, and shows:  

+ left-hand edge: percent deviance explained when run in a GLM (or GAM, depending on convergence)
+ top edge: the number of correlations higher than 0.7 between this input and all the others in the matrix
+ left-most graph: response curve, with lower rug=absence points, upper rug=presence points
+ diagonal graph: histogram distribution of the input, titled with the name of the input
+ above diagonal: the greater of the Pearson, Spearman, or Kendall pairwise correlation between the row input and the column input
+ below diagonal: pairwise scatterplot of the row input and the column input (red=presence, blue=absence)  

![Generated by VisTrails SAHM](CovariateCorrelationDisplay_Feb24.png)

## ROC Plots

*Note:* I've included the metrics for the "Climate and Topo/Soils" models (i.e., no land-cover) for comparison, but I only ran the cross-validation on the models that included land-cover.  

```{r roc, cache=FALSE, results="hold", collapse=TRUE, out.width="50%", fig.width=4.5, fig.align="default", fig.show="hold"}
evRF_cd[1]
evGLM_cd[1]

evRF_all1[1]
evGLM_all[1]

evRF_all2[1]
evGLM_all2[1]
```


## Evaluation Metrics

The **'Threshold'** value shown is the output value at which *Sensitivity = Specificity*.  

*Note:* I've included the metrics for the "Climate and Topo/Soils" models (i.e., no land-cover) for comparison, but I only ran the cross-validation on the models that included land-cover. 

```{r, echo=FALSE, cache=FALSE}
# No doubt this would be better in a loop, but no time for code elegance!
# Have an outrageously gross pipe-alanche instead!
outdf <- data.frame(Model = evRF_cd[[2]][1], AUC = as.numeric(evRF_cd[[2]][2]), TSS = as.numeric(evRF_cd[[2]][3]), 
                    Error_Rate = as.numeric(evRF_cd[[2]][4]), Kappa = as.numeric(evRF_cd[[2]][5]), 
                    PCC = as.numeric(evRF_cd[[2]][6]),
                    Sensitivity = as.numeric(evRF_cd[[2]][7]), Specificity = as.numeric(evRF_cd[[2]][8]),
                    Threshold = as.numeric(evRF_cd[[2]][9]))

outdf <- outdf %>% add_row(Model = evGLM_cd[[2]][1], AUC = as.numeric(evGLM_cd[[2]][2]), 
                           TSS = as.numeric(evGLM_cd[[2]][3]), Error_Rate = as.numeric(evGLM_cd[[2]][4]), 
                           Kappa = as.numeric(evGLM_cd[[2]][5]), PCC = as.numeric(evGLM_cd[[2]][6]),
                           Sensitivity = as.numeric(evGLM_cd[[2]][7]), Specificity = as.numeric(evGLM_cd[[2]][8]),
                           Threshold = as.numeric(evGLM_cd[[2]][9])) %>% 
  add_row(Model = evRF_all1[[2]][1], AUC = as.numeric(evRF_all1[[2]][2]), 
          TSS = as.numeric(evRF_all1[[2]][3]), Error_Rate = as.numeric(evRF_all1[[2]][4]), 
          Kappa = as.numeric(evRF_all1[[2]][5]), PCC = as.numeric(evRF_all1[[2]][6]),
          Sensitivity = as.numeric(evRF_all1[[2]][7]), Specificity = as.numeric(evRF_all1[[2]][8]),
          Threshold = as.numeric(evRF_all1[[2]][9])) %>%
  add_row(Model = evGLM_all[[2]][1], AUC = as.numeric(evGLM_all[[2]][2]), 
          TSS = as.numeric(evGLM_all[[2]][3]), Error_Rate = as.numeric(evGLM_all[[2]][4]), 
          Kappa = as.numeric(evGLM_all[[2]][5]), PCC = as.numeric(evGLM_all[[2]][6]),
          Sensitivity = as.numeric(evGLM_all[[2]][7]), Specificity = as.numeric(evGLM_all[[2]][8]),
          Threshold = as.numeric(evGLM_all[[2]][9])) %>%
  add_row(Model = evRF_all2[[2]][1], AUC = as.numeric(evRF_all2[[2]][2]), 
          TSS = as.numeric(evRF_all2[[2]][3]), Error_Rate = as.numeric(evRF_all2[[2]][4]), 
          Kappa = as.numeric(evRF_all2[[2]][5]), PCC = as.numeric(evRF_all2[[2]][6]),
          Sensitivity = as.numeric(evRF_all2[[2]][7]), Specificity = as.numeric(evRF_all2[[2]][8]),
          Threshold = as.numeric(evRF_all2[[2]][9])) %>%
  add_row(Model = evGLM_all2[[2]][1], AUC = as.numeric(evGLM_all2[[2]][2]), 
          TSS = as.numeric(evGLM_all2[[2]][3]), Error_Rate = as.numeric(evGLM_all2[[2]][4]), 
          Kappa = as.numeric(evGLM_all2[[2]][5]), PCC = as.numeric(evGLM_all2[[2]][6]),
          Sensitivity = as.numeric(evGLM_all2[[2]][7]), Specificity = as.numeric(evGLM_all2[[2]][8]),
          Threshold = as.numeric(evGLM_all2[[2]][9]))

```

### Metrics for the full model runs
```{r, message=FALSE, cache=FALSE}
kable(outdf, format = "html", row.names = FALSE, digits = 3) %>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed", "responsive")) %>%
  column_spec(2:9, color="black") %>% column_spec(1, bold=TRUE)
```

### Cross-fold validation metrics
A k-fold split number was randomly assigned to each Grid_ID. There are 15,185 unique Grid_IDs in the dataset, so each fold contains ~1,518 sampling grid cells. While the technique used selects whole cells (i.e., all the presence/absence points within the cell), it selects them randomly across the region and not stratified geographically. Also note that the number of presence vs. absence points in each fold is *not* equal, but the results were checked and look reasonable.  

**Mean of 10 folds:**  
```{r, message=FALSE, cache=FALSE}
# These metrics were previously created in kfold.r
cv_metrics <- read.csv("CrossValidation_metrics.csv")
mnmetrics <- cv_metrics %>% select(-fold) %>%
  group_by(model) %>%
  summarize_all(.funs=mean)

kable(mnmetrics, format = "html", row.names = FALSE, digits = 3) %>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed", "responsive")) %>%
  column_spec(2:9, color="black") %>% column_spec(1, bold=TRUE)
```

*Note:* The model names are getting long and confusing. Here's how they map out with the above table;  

+ GLMER_alltypes_Feb25 = GLMM Climate, Topo/Soils, NLCD  
+ GLMER_alltypes_Mar02 = GLMM Climate, Topo/Soils, NLCD formula modified  

+ RF_alltypes_onehot_Feb25 = RF Climate, Topo/Soils, NLCD One-Hot  
+ RF_alltypes_Mar02 = RF Climate, Topo/Soils, NLCD (as factor)  

 
```{r, cache=FALSE, fig.width=8}
cv <- as.data.table(cv_metrics)
tograph <- melt(cv, id.vars = c("model", "fold"), measure.vars = c(3, 4, 6:9), variable.name = "metric")
qplot(factor(metric), value, data=tograph, geom="boxplot", color=model, ylim = c(0.3,1),
      xlab = "Performance Metric", main = "Cross-validation Performance Metrics per Model")
```


## Variable Importance

*Note:* I've included the metrics for the "Climate and Topo/Soils" models (i.e., no land-cover) for comparison, but I only ran the cross-validation on the models that included land-cover. 

### 1) Climate + Topo/Soils + Interactions (for GLMM anyway)
  
#### Random Forest
```{r, cache=FALSE, results="hold"}
varImpPlot(RF_cd, main = evRF_cd[[2]][1])
ptab_rf <- table(RF_cd$predicted, RF_cd$y)

print("Confusion Matrix")
getConfusionMatrix(ptab_rf)
```
  
#### GLMM
```{r, cache=FALSE, results="hold", message=FALSE}
summary(GLM_cd)

print("Confusion Matrix")
evGLM_cd[3]
```

### 2) Climate + Topo/Soils + Land Use + Interactions
  
#### Random Forest with One-Hot NLCD
```{r, cache=FALSE, results="hold"}
varImpPlot(RF_all1, main = evRF_all1[[2]][1])
ptab_rf <- table(RF_all1$predicted, RF_all1$y)

print("Confusion Matrix")
getConfusionMatrix(ptab_rf)
```
  
#### GLMM with Original Interations
```{r, cache=FALSE, results="hold", message=FALSE}
summary(GLM_all)

print("Confusion Matrix")
evGLM_all[3]
```

### 3) Tweaks to the above models
  
#### Random Forest with NLCD as factor, not one-hot
```{r, cache=FALSE, results="hold"}
varImpPlot(RF_all2, main = evRF_all2[[2]][1])
ptab_rf <- table(RF_all2$predicted, RF_all2$y)

print("Confusion Matrix")
getConfusionMatrix(ptab_rf)
```
  
#### GLMM with modified interaction terms and GDD5 as quadratic
```{r, cache=FALSE, results="hold", message=FALSE}
summary(GLM_all2)

print("Confusion Matrix")
evGLM_all2[3]
```
