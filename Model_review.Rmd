---
title: "HOTR model evaluation"
author: "Michelle M. Fink"
date: 10/24/2019
output: 
  html_notebook: 
    code_folding: hide
    fig_height: 6
    fig_width: 6.5
    highlight: tango
    theme: readable
---
Load libraries and functions
```{r message=FALSE, warning=FALSE}
library(dplyr)
library(stringr)
library(randomForest)
library(ROCR)
library(PresenceAbsence) # for Kappa

getConfusionMatrix <- function(tbl) {
  #https://stats.stackexchange.com/questions/35609/why-do-i-need-bag-composition-to-calculate-oob-error-of-combined-random-forest-m/35613#35613
  class.error = vector()
  
  for (i in 1:nrow(tbl)) {
    rowSum = sum(tbl[i,])
    accurate = diag(tbl)[i]
    error = rowSum - accurate
    
    class.error[i] = error / rowSum
  }   
  return(cbind(tbl, class.error))
}

opt.cut <- function(perf, pred){
  #https://www.r-bloggers.com/a-small-introduction-to-the-rocr-package/
  #calculates sensitivity = specificity threshold
  mapply(FUN=function(x, y, p){
    d = (x - 0)^2 + (y-1)^2
    ind = which(d == min(d))
    c(sensitivity = y[[ind]], specificity = 1-x[[ind]], 
      cutoff = p[[ind]])
  }, perf@x.values, perf@y.values, pred@cutoffs)
}
```

List of Completed Models:
```{r}
pth <- "H:/HOTR_models/"
RF1 <- "RF_Oct21_full.rds"
RF2 <- "RF_Oct22_full.rds"
GLM1 <- "GLMER_Oct22_full.rds"

```

## Calculate Evaluation Statistics
For each model

Let's load and unload models as we go, because they are large files.  

### Random Forest

```{r}
rf.mod1 <- readRDS(paste0(pth,RF1))
rf.mod2 <- readRDS(paste0(pth,RF2))
```

```{r}
# Add components that weren't filled in due to randomForest::combine
#https://stats.stackexchange.com/questions/35609/why-do-i-need-bag-composition-to-calculate-oob-error-of-combined-random-forest-m/35613#35613
ptab_rf.mod1 <- table(rf.mod1$predicted, rf.mod1$y)
err_rate <- 1-sum(diag(ptab_rf.mod1))/sum(ptab_rf.mod1)
rf.mod1$confusion <- getConfusionMatrix(ptab_rf.mod1)
print(RF1)
print(rf.mod1)
str_glue("OOB estimate of  error rate: {round(err_rate * 100, 3)} % ")

p_rf.mod1 <- predict(rf.mod1, type = "prob")
pred_rf.mod1 <- prediction(p_rf.mod1[,2], rf.mod1$y)
perf.roc1 <- performance(pred_rf.mod1, "tpr", "fpr")
AUC <- performance(pred_rf.mod1, measure = "auc")@y.values[[1]]
sst <- opt.cut(perf.roc1,pred_rf.mod1)
TSS <- sst[1] + sst[2] - 1

#Presence.Absence insists on its own data structure
padat <- data.frame(ID=seq(1:length(pred_rf.mod1@predictions[[1]])),
                    OBS=as.numeric(levels(pred_rf.mod1@labels[[1]]))[pred_rf.mod1@labels[[1]]],
                    PRED=pred_rf.mod1@predictions[[1]])

pacmx <- cmx(padat, threshold = sst[3])
kappa <- Kappa(pacmx, st.dev = FALSE)
PCC <- pcc(pacmx, st.dev = FALSE)

outdf <- data.frame(Model = RF1, AUC = AUC, TSS = TSS, 
                    Kappa = kappa, PCC = PCC,
                    Sensitivity = sst[1], Specificity = sst[2],
                    Error_Rate = err_rate, Threshold = sst[3])

```
Start plotting stuff.
```{r}
# Variable Importance
varImpPlot(rf.mod1, main=paste(RF1, "Variable Importance"), pch=19, color="darkblue", cex=0.9)
varImpPlot(rf.mod2, main=paste(RF2, "Variable Importance"), pch=19, color="darkblue", cex=0.9)

# ROC plot1
i <- which(perf.roc1@alpha.values[[1]] == sst[3])
plot(perf.roc1, lwd= 3, col="red", 
     main=paste(RF1, "ROC"),
     sub=str_glue("AUC={round(AUC,3)}, Threshold={round(sst[3],3)}"))
abline(a=0,b=1,lwd=2,lty=2,col="gray")
points(x=perf.roc1@x.values[[1]][i], y=perf.roc1@y.values[[1]][i], pch=19, col="blue")
```


```{r}
# 2nd model
ptab_rf.mod2 <- table(rf.mod2$predicted, rf.mod2$y)
err_rate <- 1-sum(diag(ptab_rf.mod2))/sum(ptab_rf.mod2)
rf.mod2$confusion <- getConfusionMatrix(ptab_rf.mod2)
print(RF2)
print(rf.mod2)
str_glue("OOB estimate of  error rate: {round(err_rate * 100, 3)} % ")

p_rf.mod2 <- predict(rf.mod2, type = "prob")
pred_rf.mod2 <- prediction(p_rf.mod2[,2], rf.mod2$y)
perf.roc2 <- performance(pred_rf.mod2, "tpr", "fpr")
AUC <- performance(pred_rf.mod2, measure = "auc")@y.values[[1]]
sst <- opt.cut(perf.roc2,pred_rf.mod2)
TSS <- sst[1] + sst[2] - 1

#Presence.Absence insists on its own data structure
padat <- data.frame(ID=seq(1:length(pred_rf.mod2@predictions[[1]])),
                    OBS=as.numeric(levels(pred_rf.mod2@labels[[1]]))[pred_rf.mod2@labels[[1]]],
                    PRED=pred_rf.mod2@predictions[[1]])

pacmx <- cmx(padat, threshold = sst[3])
kappa <- Kappa(pacmx, st.dev = FALSE)
PCC <- pcc(pacmx, st.dev = FALSE)

outdf <- outdf %>% add_row(Model = RF2, AUC = AUC, TSS = TSS,
                           Kappa = kappa, PCC = PCC,
                           Sensitivity = sst[1], Specificity = sst[2],
                           Error_Rate = err_rate, Threshold = sst[3])
# ROC plot2
i <- which(perf.roc2@alpha.values[[1]] == sst[3])
plot(perf.roc2, lwd= 3, col="red", 
     main=paste(RF2, "ROC"),
     sub=str_glue("AUC={round(AUC,3)}, Threshold={round(sst[3],3)}"))
abline(a=0,b=1,lwd=2,lty=2,col="gray")
points(x=perf.roc2@x.values[[1]][i], y=perf.roc2@y.values[[1]][i], pch=19, col="blue")

```

Lighten the load
```{r}
rm(pred_rf.mod1)
rm(rf.mod1)
rm(pred_rf.mod2)
rm(rf.mod2)
```


### Generalized Linear Mixed Model
```{r}
glm.mod1 <- readRDS(paste0(pth,GLM1))
```
This was the function call (using package **lme4**):  

> glmm.fit1 <- glmer(f_glm, data = indata, family = "binomial", control = glmerControl(optimizer = "bobyqa"), nAGQ = 0)  

See output below for formula used.
```{r warning=FALSE}
summary(glm.mod1)
```

```{r warning=FALSE}
#p_glm.mod1 <- predict(glm.mod1, type = "response")
pred_glm.mod1 <- prediction(glm.mod1@resp$mu, glm.mod1@resp$y)
perf.roc3 <- performance(pred_glm.mod1, "tpr", "fpr")
perf.err <- performance(pred_glm.mod1, "err")
AUC <- performance(pred_glm.mod1, measure = "auc")@y.values[[1]]
sst <- opt.cut(perf.roc3,pred_glm.mod1)
TSS <- sst[1] + sst[2] - 1
err_rate <- perf.err@y.values[[1]][which(perf.err@x.values[[1]] == sst[3])]

#Presence.Absence insists on its own data structure
padat <- data.frame(ID=seq(1:length(pred_glm.mod1@predictions[[1]])),
                    OBS=as.numeric(levels(pred_glm.mod1@labels[[1]]))[pred_glm.mod1@labels[[1]]],
                    PRED=pred_glm.mod1@predictions[[1]])

pacmx <- cmx(padat, threshold = sst[3])
kappa <- Kappa(pacmx, st.dev = FALSE)
PCC <- pcc(pacmx, st.dev = FALSE)

outdf <- outdf %>% add_row(Model = GLM1, AUC = AUC, TSS = TSS,
                           Kappa = kappa, PCC = PCC,
                           Sensitivity = sst[1], Specificity = sst[2],
                           Error_Rate = err_rate, Threshold = sst[3])
# ROC plot3
i <- which(perf.roc3@alpha.values[[1]] == sst[3])
plot(perf.roc3, lwd= 3, col="red", 
     main=paste(GLM1, "ROC"),
     sub=str_glue("AUC={round(AUC,3)}, Threshold={round(sst[3],3)}"))
abline(a=0,b=1,lwd=2,lty=2,col="gray")
points(x=perf.roc3@x.values[[1]][i], y=perf.roc3@y.values[[1]][i], pch=19, col="blue")
```
```{r include=FALSE}
# Don't think this is worth it.
#calib <- calibration.plot(padat, N.bins = 500)
```
```{r eval=FALSE, include=FALSE}
# plot(calib$BinObs ~ calib$BinPred, ylab="Probability of presence",
#      xlab="Predicted probability of presence", col="darkgreen", pch=20,
#      main=paste(GLM1, "Calibration Plot"), asp=1, xlim=c(0,1), xaxs="i")
# abline(a=0,b=1,lwd=2,lty=2,col="gray")
```

**Metrics so far**
```{r paged.print=TRUE}
outdf

```